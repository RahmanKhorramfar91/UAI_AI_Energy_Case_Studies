{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SiJC5mqFpg47",
        "8AqTEsITmGZN",
        "2l2TBTgdnANB"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahmanKhorramfar91/UAI_AI_Energy_Case_Studies/blob/main/1%20-%20Load%20Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Case Study 1 - Universal AI, AI and Sustainability: Energy**\n"
      ],
      "metadata": {
        "id": "DpzCOU_CI957"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This case study is prepared by [Rahman Khorramfar](https://www.rahmankhorramfar.com/) for the ***AI and Sustainability: Energy*** module instructed by Professor [Saurabh Amin](https://cee.mit.edu/people_individual/saurabh-amin/). The module is part of the Universal AI course.\n",
        "\n",
        "My email for questions, suggestions, and comments: khorram@mit.edu"
      ],
      "metadata": {
        "id": "5pPVX4L6sCQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the Case Study\n",
        "\n",
        "This case study supplements the lecture material by providing basic implementations for the main forecasting methods, and requires the learners to develop an effective forecasting framework for the medium- and long-term projection of electric power demand for the region of study.\n",
        "\n",
        "In the implementation part, we are interested in a medium-term forecast for the hourly demand over one week. The forecasting horizon is assumed to be the last week of a year, so that all previous weeks are considered as historical records. We cover baseline forecasting methods, ARIMA/SARIMA, XGBoost, and LSTM/GRU techniques; and briefly mention accuracy measures, hyperparameter tuning, and feature engineering.\n",
        "\n",
        "The main task for learners is to develop effective forecasting models for the medium- and long-term projection of electric power demand for the region covered by [ISO New England (ISONE)](https://www.iso-ne.com/) which encompasses six US northeastern states including Maine, New Hampshire, Vermont, Massachusetts, Rhode Island, and Connecticut. The long-term forecast involves projecting demand for a future target year, thus it results in hourly demand time series for an entire year based on historical data."
      ],
      "metadata": {
        "id": "R9QhZRUR81Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of the content**:\n",
        "\n",
        "1. [Definitions and Basic Concepts](#definitions)\n",
        "2. [Getting Started](#getting_started)\n",
        "3. [Baseline: Moving Average (Rolling Average)](#baseline)\n",
        "4. [ARIMA/SARIMA](#arima)\n",
        "5. [XGBoost](#xgboost)\n",
        "6. [LSTM and GRU](#lstm)\n",
        "7. [Case Study Questions](#questions)\n",
        "9. [References and Further Reading](#references)"
      ],
      "metadata": {
        "id": "BWZ9zB_B--ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>1. Definitions and Basic Concepts </font>** [<a name=\"definitions\"></a>](https://)\n",
        "\n",
        "## Definitions\n",
        "\n",
        "- **Forecasting**: is a common task in energy systems and refers to the process of predicting the future parameters, given any relevant knowledge of the past. Forecasting helps to inform decisions in all levels of energy systems planning and optimization. It is roughly categorized into very-short-term (minutes to hours), short-term (hours to day), medium-term (days to weeks), and long-term (weeks to years).  \n",
        "\n",
        "- **Time Series**: a set of time-indexed data points where each point represents a measurement or observation at a specified time. A time series is *univariate* if at each time period a single data point is recorded, and is *multivariate* otherwise.\n",
        "\n",
        "- **Stationary**: A time series is stationary if the joint distribution over any set of consecutive samples remains the same. In other words, the statistical properties (mean, standard deviation, autocovariance) of the time series do not change over time.\n",
        "\n",
        "- **Trend**: underlying pattern in non-stationary time series and refers to long-term gradual changes in the data (linear upward, linear downward, nonlinear)\n",
        "\n",
        "- **Seasonality**: refers to regular and repeating patterns occurring at fixed intervals (daily, weekly, etc.)\n",
        "\n",
        "- **Response variable**: is the predicted variable. This variable is also known as dependent, target, internal, forecast, predicted, observed, Y-variable, and endogenous variable.\n",
        "\n",
        "- **Covariate**: variable(s) help predict or explain the response variable. These variables also known as predictors, regressors, explanatory, exogenous, external, X-variable, and feature variables.\n",
        "\n",
        "- **َAutocorrelation**: correlation between a variable at a specific time period and its lagged values. The analysis of autocorrelation helps to determine pattern, dependencies, and seasonality in the data.\n"
      ],
      "metadata": {
        "id": "SiJC5mqFpg47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Metrics and Evaluation Criteria\n",
        "\n",
        "Depending on the context and applications, different metrics can be used to measure the performance of a forecasting model. Some of the widely used metrics are Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). The impact of the forecasted points on the downstream task can also be used as an indication of the model's performance, as we will see in this case study.  \n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "5Kmpj1BpWyy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>2. Getting Started </font>**  <a name=\"getting_started\"></a>\n",
        "This dataset for this case study is based on the historical data from the New England region of the US. The data is available at the [ISO New England (ISONE) website](https://www.iso-ne.com/system-planning/planning-models-and-data/variable-energy-resource-data/) under *2024_isone_variable_energy_resource_ver_data_series_2000-2023_rev1*. The dataset contains hourly time series data for load, wind, and solar between 2000 and 2023 (24 years) for 8 New England zones, as well as the aggregated values. for the entire region. Here, we selected the aggregated load time series for ISONE and curated the data slightly for better readibility. The curated data is available at [the GitHub repository](https://github.com/RahmanKhorramfar91/UAI_AI_Energy_Case_Studies) created to store data and codes for all the case studies.\n",
        "\n",
        "We will begin by summoning the required packages and continue with recalling and processing the data. For each technique, a very brief description is provided."
      ],
      "metadata": {
        "id": "PTjjAc0F_HxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the required packages\n",
        "include packages for data science and scientific computing, specialized forecasting, deep learning, file management, and operating system packages."
      ],
      "metadata": {
        "id": "or9Y3EM3IYnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VIq33DEnJ2Ns"
      },
      "outputs": [],
      "source": [
        "import numpy as np; # for scientific computing\n",
        "import pandas as pd; # for data science\n",
        "import matplotlib.pyplot as plt; # for plotting\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose; # for statistical models including forecasting\n",
        "from statsmodels.tsa.stattools import adfuller;\n",
        "from statsmodels.tsa.arima.model import ARIMA;\n",
        "import xgboost as xgb; # for XGBoos technique\n",
        "from xgboost import plot_importance, plot_tree;\n",
        "import os; # os: for directory control\n",
        "import zipfile; # to handle zip files\n",
        "import tensorflow as tf; # tensorflow\n",
        "from tensorflow.keras.models import Sequential;\n",
        "from tensorflow.keras.layers import *;\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint;\n",
        "from tensorflow.keras.losses import MeanSquaredError;\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError;\n",
        "from tensorflow.keras.optimizers import Adam;\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf; # for ACF and PACF plots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Unzip the Data\n",
        "Clone the curated load data from GitHub repositorty for the case studies. Unzip the file in a designated folder."
      ],
      "metadata": {
        "id": "VRrzMlfLMmPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/RahmanKhorramfar91/UAI_Case_Studies.git';"
      ],
      "metadata": {
        "id": "xwAbYqmiNxoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67bccc3-bc02-44bd-b9ed-f39a79ecb353"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UAI_Case_Studies'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = os.getcwd()+'/UAI_Case_Studies/load_time_series_data.zip';\n",
        "extract_dir = os.getcwd()+'/load_time_series_data';\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "ajSFZSGRlDUl",
        "outputId": "8173a417-9ed5-4fcb-f582-e6a77aee0446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/UAI_Case_Studies/load_time_series_data.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1903685091.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzip_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/UAI_Case_Studies/load_time_series_data.zip'\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mextract_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/load_time_series_data'\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/UAI_Case_Studies/load_time_series_data.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CWcNZH2QNElU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preview and Preprocessing\n",
        "Read the data for a particular year and add some temporal features (week of the year, day of week, etc.) to the dataset."
      ],
      "metadata": {
        "id": "nFziPwGNOz34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yr = 2023;\n",
        "zone = 'ISONE';\n",
        "data = pd.read_csv(f'/content/load_time_series_data/load_time_series_data/{yr}_{zone}_load.csv');\n",
        "data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "4IKMfJPtIm4o",
        "outputId": "5dd29763-b3fc-461a-ab54-7575106523b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1735570023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2023\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mzone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ISONE'\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/load_time_series_data/load_time_series_data/{yr}_{zone}_load.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add temporal features\n",
        "data['hour_of_year'] = data.index+1;\n",
        "data.index = pd.to_datetime(data.date);\n",
        "data['date'] = data.index;\n",
        "data['year'] = data['date'].dt.year;\n",
        "data['day_of_week'] = data['date'].dt.dayofweek;\n",
        "data['quarter_of_year'] = data['date'].dt.quarter;\n",
        "data['month'] = data['date'].dt.month;\n",
        "data['day_of_year'] = data['date'].dt.dayofyear;\n",
        "data['week_of_year'] = data['date'].dt.dayofyear//7 + 1;"
      ],
      "metadata": {
        "id": "K09yYZAmIsbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Plotting and Evaluation Functions"
      ],
      "metadata": {
        "id": "mtqd_fvR5A9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_accuracy_metrics(test, forecasted):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    test : numpy array, the vector of test set (real positive values)\n",
        "    forecasted : numpy array, the forecasted values (real positive values)\n",
        "    OUTPUT: three commonly used accuracy metrics, namely RMSE, MAE, and MAPE\n",
        "    '''\n",
        "    if test.shape!= forecasted.shape:\n",
        "      test = test.reshape(-1);\n",
        "      forecasted = forecasted.reshape(-1);\n",
        "\n",
        "    # calculate error metrics\n",
        "    RMSE = np.sqrt(np.mean((test - forecasted)**2));\n",
        "    MAE = np.mean(np.abs(test - forecasted));\n",
        "    MAPE = np.mean(np.abs((test - forecasted) / test))*100;\n",
        "\n",
        "    print(f'Point Accuracy Metrics\\n \\t RMS: {np.round(RMSE,2)}, MAE: {np.round(MAE,2)}, MAPE: {np.round(MAPE,2)}');\n",
        "    return RMSE, MAE, MAPE;"
      ],
      "metadata": {
        "id": "lqJglIaG5TXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_and_report_accuracy_metrics(test, forecasted, days_to_show):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    test : numpy array, the vector of test set (real positive values)\n",
        "    forecasted : numpy array, the forecasted values (real positive values)\n",
        "    days_to_show : number of days to contrast the hourly date between actual and forecasted values\n",
        "\n",
        "    plot the actual vs. forecasted values for the next 'days_to_show' number of days\n",
        "    and report the three commonly used accuracy metrics, namely RMSE, MAE, and MAPE\n",
        "    '''\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4),\n",
        "                            gridspec_kw={'width_ratios': [10],'height_ratios': [5],\n",
        "                                        'wspace': 0.1,'hspace': 0.1});\n",
        "\n",
        "    ax.plot(np.arange(days_to_show*24), test[:days_to_show*24], color='xkcd:warm blue', label='Actual')\n",
        "    ax.plot(np.arange(days_to_show*24), forecasted[:days_to_show*24], color='xkcd:vermillion', label='Forecasted')\n",
        "\n",
        "    ax.set_xlabel('Hour');\n",
        "    ax.set_ylabel('Gross Load (MW)',fontsize=14);\n",
        "    ax.legend(loc='upper center',ncol=2, bbox_to_anchor=(0.5, 1), prop={'size': 12});\n",
        "\n",
        "    if test.shape!= forecasted.shape:\n",
        "      test = test.reshape(-1);\n",
        "      forecasted = forecasted.reshape(-1);\n",
        "\n",
        "    # calculate error metrics\n",
        "    RMSE = np.sqrt(np.mean((test - forecasted)**2)); # Root mean square deviation\n",
        "    MAE = np.mean(np.abs(test - forecasted)); # Mean absolute error\n",
        "    MAPE = np.mean(np.abs((test - forecasted) / test))*100; # Mean absolute percentage error\n",
        "\n",
        "    print(f'Point Accuracy Metrics\\n \\t RMSE: {np.round(RMSE,2)}, MAE: {np.round(MAE,2)}, MAPE: {np.round(MAPE,2)}%');"
      ],
      "metadata": {
        "id": "ftqGi7e25H9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "_XDporCp8dMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>3. Baseline: Moving Average (Rolling Average) </font>** <a name=\"baseline\"></a>\n",
        "Moving Average (MA) methods are used for forecasting future values based on the average of a set of past data. MA methods are used for short-term forecasting, usually less than the next 4 periods. Nevertheless, we will apply it to forecast for the next one week, just to provide a baseline of comparison with other methods.\n",
        "\n",
        "The **Simple Moving Average** (SMA) forecasting formula predicts the next value based on the average of the previous $p$ values in the time series:\n",
        "\n",
        "$\\hat{y}_{t} = \\frac{1}{p} \\sum_{i=1}^{p} y_{t-i}$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ \\hat{y}_{t}$: forecasted value at time $t$\n",
        "- $y_{t-i}$: past observations\n",
        "- $p$: window size\n",
        "\n",
        "Closely related is the **Weighted Moving Average** (WMA) method:\n",
        "\n",
        "$\\hat{y}_{t} = \\frac{1}{p} \\sum_{i=1}^{p}w_i y_{t-i}$\n",
        "\n",
        " which weights the observations with $w_\\tau$. A special case of WMA is **Exponential Smoothing** defined as:\n",
        "\n",
        " $\\hat{y}_{t} = \\frac{1}{p} \\sum_{i=1}^{p} \\rho^{i} y_{t-i}$\n",
        "\n"
      ],
      "metadata": {
        "id": "8AqTEsITmGZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "3skkHtI6aPs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets for both load and selected covariates\n",
        "train_size = 51*7*24; # medium term forecasting for the one week (last week) of the year\n",
        "exog_columns = ['temperature_C', 'global_horizontal_irradiance', '10m_wind_speed','day_of_year','hour_of_year','week_of_year'];\n",
        "train_load, test_load = data['gross_load_MW'][:train_size], data['gross_load_MW'][train_size:];\n",
        "train_exog, test_exog = data[exog_columns][:train_size], data[exog_columns][train_size:];\n",
        "print(type(test_load));"
      ],
      "metadata": {
        "id": "y2c4DXOPvOB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the simple moving average\n",
        "window_size = 24;\n",
        "forecast_period = len(test_load);\n",
        "forecasted = np.zeros(forecast_period);\n",
        "for t in range(len(forecasted)):\n",
        "    s1 = 0;\n",
        "    if window_size>t:\n",
        "      s1 += train_load[-window_size+t:].sum()\n",
        "    s1 += np.sum(forecasted[max(0,t-window_size):t]);\n",
        "    forecasted[t] = s1/window_size;\n",
        "\n",
        "plot_and_report_accuracy_metrics(test_load.to_numpy(), forecasted, 7);"
      ],
      "metadata": {
        "id": "zKbQYuwvLMLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**<font color='red'>Question 1: </font>** extend the method for weighted average and average smoothing methods.\n",
        "\n",
        "how these models be modified to handle seasonality and trends?\n",
        "\n",
        "how to make these models to work for multivariate forecasting?\n",
        "\n",
        "provide another baseline with multivariate regression\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "2CZ3G4YbnIaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>4. ARIMA/SARIMA </font>** <a name=\"arima\"></a>\n",
        " ARIMA and its seasonal variant, SARIMA are among the most notable statistical forecasting models. Here, we first introduce their building blocks and then introduce some heuristic rules to tune their hyperparameters. We wrap this section with the Python implementation and the ensuing questions.\n",
        "\n",
        "\n",
        "Let's start with\n",
        "\n",
        "- Large p and q can result in overfitting\n",
        "\n",
        "https://towardsdatascience.com/time-series-models-d9266f8ac7b0/\n",
        "\n",
        "https://r4ds.github.io/bookclub-fpp/difference-between-stochastic-and-deterministic-trends.html\n",
        "\n"
      ],
      "metadata": {
        "id": "FNNtG6D2Qv1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Blocks\n",
        "ARIMA/SARIMA stands for (Seasonal) AutoRegressive Integrated Moving Average. The model is composed of two component, namely autoregressive and moveing average, and has one feature, namely integrated. The underlying assumption in many statistical forecasting models is the staionarity of the data.\n",
        "\n",
        "**Autoregressive (AR)** component that models the relationship between the future forecast and its past observation. AR component is usually linear function of a constant $c$, weighted combination of past observations up to order $p$, and white noise $\\epsilon_t$. The term *white noise* refers to a sequence of random variables that are uncorrelated, have a mean of zero, and a constant variance.\n",
        "\n",
        "$y_{t} = c+ \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\epsilon_t$\n",
        "\n",
        "Autoregressive model of order $p$ is denoted by $AR(p)$.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Moving Average (MA)** similar to AR but instead of past observation, past forecast errors are used:\n",
        "\n",
        "$y_{t} = c+ \\sum_{i=1}^{q}\\beta_i \\epsilon_{t-i} + \\epsilon_t$\n",
        "\n",
        "Moving average model of order $q$ is denoted as $MV(q)$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Integrated** feature differences two consecutive data points in an attempt to make the time series stationary. This feature can only make the statistical average (mean) constant. However, if the variance still remain dynamic other methods such as Box-Cox transformation is applied which is not covered here. The number of times the data is differences is denoted by $d$ parameter.\n"
      ],
      "metadata": {
        "id": "cMYy7E0Tf0ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARIMA Model\n",
        "\n",
        "ARIMA combines the AR and MV with itegrated feature. Therefore, letting $y' = y_t-y_{t-1}$ to represent the integrated feature, the the ARIMA$(p, d, q)$ model is\n",
        "\n",
        "$y'_{t} = c+ \\sum_{i=1}^{p}\\alpha_i y'_{t-i} + \\sum_{i=1}^{q}\\beta_i \\epsilon_{t-i} + \\epsilon_t$\n",
        "\n"
      ],
      "metadata": {
        "id": "5GlFsqOZwD6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ACF and PACF\n",
        "Autocorrelation quantifies the linear relationship between a time series and its past values (lagged values). The analysis of autocorrelation is fundamental for model selection in ARIMA/SARIMA. Two widely used autocorrelation-based measures are autocorrelation function (ACF) and partial autocorrelation function (PACF).\n",
        "\n",
        "**ACF** quantifies the correlation of a time series with its lags ($y_t$ and $y_{t-k}$, for $k=1, 2, \\ldots$)\n",
        "\n",
        "$r_k = \\frac{Cov(y_y, y_{t-k})}{\\sqrt{Var(y_t)Var(y_{t-k})}} = \\frac{\\sum_{t=k+1}^{T}(y_t-\\bar{y})(y_{t-k}-\\bar{y})}{\\sum_{t=1}^{T}y_t-\\bar{y}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "**PACF** quantifies the correlation of time series with its lags, removing the influence of intermediate lags. For example, if $y_t$ and $y_{t-1}$, and $y_{t-1}$ and $y_{t-1}$ are correlatd, the partial correlation between $y_{t-2}$ and $y_t$ is the quantity that is not explain by their relationship with $y_{t-1}$.\n",
        "\n",
        "\n",
        "$r'_k = \\frac{Cov(y_y, y_{t-k}|y_{t-1}, y_{t-2}, \\ldots, y_{t-k+1})}{\\sqrt{Var(y_t|y_{t-1}, y_{t-2}, \\ldots, y_{t-k+1})Var(y_{t-k}|y_{t-1}, y_{t-2}, \\ldots, y_{t-k+1})}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "**ACF and PACF Plots**: these plots help identify the patterns (stationarity, seasonality and trend) and dependencies (order of $p$ and $q$ values in the AR and MV models). There are some rule of thums to help with the anlysis and model selection. Given a stationary model, there are some rule of thums to choose possible values for $p$ and $q$ in ARIMA/SARIMA:\n",
        "- ARIMA($p, d, 0$) is suggested if ACF is decaying gradually and PACF spikes at lag $p$ but not beyond\n",
        "- ARIMA($0, d, q$) is suggested if PACF is decaying gradually and ACF spikes at lag $q$ but not beyond\n",
        "-  The lag at which the ACF plot is cut off (beyong which become insignificant) is $q$\n",
        "- The lag at which the PACF plot is cut off is $p$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JZvavbZZzJtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for Staionarity\n",
        "\n",
        "There are two main ways to check the stationary of a time series including\n",
        "\n",
        "1. **Visual inspection**: trend and seasonality are signs of non-stationarity. Moreover, a decaying or fluctuating ACF plots indicates trend and seasonality.\n",
        "\n",
        "2. **Statistical test**: Augmented Dickey-Fuller (ADF) and Kwiatkowski–Phillips–Schmidt–Shin (KPSS) tests are commonly used metrics."
      ],
      "metadata": {
        "id": "XyMUv29UzQe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARIMA Model\n",
        "Although ARIMA can be effective for timer series with seasonality, its functionality may degrade for longer seasonal cycles. SARIMA, instead, explicitly models seasonality potentially leading to better accuracy and interpretabilty.\n",
        "\n",
        "$y'_{t} = c+ \\sum_{i=1}^{p}\\alpha_i y'_{t-i} + \\sum_{i=1}^{q}\\beta_i \\epsilon_{t-i} + \\sum_{i=1}^{P}\\gamma_i y'_{t-mi} + \\sum_{i=1}^{Q}\\delta_i y_{t-mi} +\\epsilon_t$\n",
        "\n",
        "where $p$ and $q$ are the same as ARIMA; $P$ and $Q$ are the oders of seasonal AR and MV, and $m$ is the length of the season. Therefore, a SARIMA model is characterized by parameters ($p, d, q$) ($P, D, Q$)$m$\n"
      ],
      "metadata": {
        "id": "-RbkPlwZzVL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Procedure, Best Practices, and Challenges\n",
        "\n",
        "Guid to apply autoregressive models:\n",
        "1. Visualize and examine the data for stationarity, trend, outliers, and missing values.\n",
        "2. If needed, transform the data stationary by stabilizing the mean and variance\n",
        "2. Use ACF and PACF plots to estimate potential values for the model parameters\n",
        "3. Apply ARIMA/SARIMA\n",
        "4. Fine tune the model by forecast-specific (RMSE, MAE, etc.) and if needed, task-baed metrics (downstream task in which the forecasted values are used). Grid search and Bayesian optimization are common fine-tuning mechanisms.\n",
        "\n",
        "Best practices:\n",
        "- Start simple and increase the complexity if required. Simpler models are easier to interpret.\n",
        "-"
      ],
      "metadata": {
        "id": "NWxCRqR_H4zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "soBc98zEQ2Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the data\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4),\n",
        "                        gridspec_kw={'width_ratios': [10],'height_ratios': [5],\n",
        "                                    'wspace': 0.1,'hspace': 0.1});\n",
        "font_size = 15;\n",
        "hours_to_plot = range(30*24, 37*24); # use range(len(data)) for the entire year\n",
        "ax.plot(data['hour_of_year'].iloc[hours_to_plot], data['gross_load_MW'].iloc[hours_to_plot],\n",
        "        color='xkcd:warm blue', label='Gross Load (MW)');\n",
        "ax.set_xlabel('Hour of Year');\n",
        "ax.set_ylabel('Gross Load (MW)', fontsize=font_size);\n",
        "\n",
        "# Create a second y-axis for temperature\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(data['hour_of_year'].iloc[hours_to_plot], data['temperature_C'].iloc[hours_to_plot],\n",
        "         color='xkcd:vermillion', label='Temperature (°C)');\n",
        "ax2.set_ylabel('Temperature (°C)', color='tab:red', fontsize=font_size);\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red');\n",
        "\n",
        "# Titles and layout\n",
        "ax.text(hours_to_plot[0], 1.04*np.max(data['gross_load_MW'].iloc[hours_to_plot]),'Gross Load and Temperature in 2020', fontsize=font_size);"
      ],
      "metadata": {
        "id": "TrE8cNXHz6Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above figure clearly shows cyclic patterns, but no obvious trend is visible. Next, we divide the time series into train and test datasets and carry out the stationarity test using ACF/PACF plots and ADF method."
      ],
      "metadata": {
        "id": "b7SgBTN9SwMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACF and PACF plots\n",
        "fig1 = plot_acf(train_load, lags=48);\n",
        "fig2 = plot_pacf(train_load, lags=48);\n",
        "# fig1.set_size_inches(10,4);\n",
        "# fig2.set_size_inches(10,4);"
      ],
      "metadata": {
        "id": "wqi5qQ4bz58X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the seasonality is obvious from ACF plot, but there is no visual sign for any trend. Let's try out ADF test."
      ],
      "metadata": {
        "id": "h72hIjgFc_NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Dickey-Fuller test on the gross load time series\n",
        "def perform_DF_test(dat):\n",
        "    adf_result = adfuller(dat, autolag='AIC');\n",
        "    # Summarize the results\n",
        "    result_description = ['ADF stats: ', 'p_value: ', 'Used lag: ','Number of observations: ','Critical values: ']\n",
        "    for ri,rd in enumerate(result_description):\n",
        "        print(f'{rd} {adf_result[ri]}');\n",
        "\n",
        "perform_DF_test(train_load);"
      ],
      "metadata": {
        "id": "2Le7lyiBz556"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the ADF test results, the p-value is small enough to reject the null hypothesis that the data is non-stationary. Nevertheless, we use differencing to see the effect."
      ],
      "metadata": {
        "id": "tga54jP-djw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_diff = train_load.diff().dropna();\n",
        "fig1 = plot_acf(train_diff, lags=48);\n",
        "fig2 = plot_pacf(train_diff, lags=48);\n",
        "perform_DF_test(train_diff);"
      ],
      "metadata": {
        "id": "fo9EljGQz53M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The differencing made the ADF to more strongly reject the stationarity of the dataset. The analysis\n",
        "\n",
        "Next, we apply ARIMA for univariate time series."
      ],
      "metadata": {
        "id": "4KC1btZje81K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA for univariate time series (only load)\n",
        "arima = ARIMA(train_load.to_numpy(), order=(12, 1, 8));\n",
        "fitted_arima = arima.fit(method_kwargs={'maxiter':10});\n",
        "\n",
        "# Perform forecasting on the training set\n",
        "forecasted = fitted_arima.predict(start=len(train_load), end=len(data)-1);\n",
        "plot_and_report_accuracy_metrics(test_load.to_numpy(), np.array(forecasted), 7);"
      ],
      "metadata": {
        "id": "R1uuHA0gfeJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we apply ARIMA for the load time series, but we also want to use the covariates available. ARIMA with exogenous variables also called ARIMAX."
      ],
      "metadata": {
        "id": "dJem0h6wQOAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA for multivariate time series (load and its selected covariates)\n",
        "arima = ARIMA(train_load.to_numpy(), order=(12,1,8), exog=train_exog.to_numpy());\n",
        "fitted_arima = arima.fit(method_kwargs={'maxiter':10});\n",
        "\n",
        "# Perform forecasting on the training set\n",
        "forecasted = fitted_arima.predict(start=len(train_load), end=len(data)-1, exog=test_exog);\n",
        "plot_and_report_accuracy_metrics(test_load.to_numpy(), forecasted, 7);"
      ],
      "metadata": {
        "id": "esbQqXOJfeGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we apply SARIMA for the load time series."
      ],
      "metadata": {
        "id": "9ywTYk2ON4V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SARIMA for univariate time series\n",
        "arima = ARIMA(train_load.to_numpy(), order=(4,1,2), seasonal_order=(1, 1, 1, 24));\n",
        "fitted_arima = arima.fit(method_kwargs={'maxiter':10});\n",
        "\n",
        "# Perform forecasting on the training set\n",
        "forecasted = fitted_arima.predict(start=len(train_load), end=len(data)-1);\n",
        "plot_and_report_accuracy_metrics(test_load.to_numpy(), forecasted, 7);"
      ],
      "metadata": {
        "id": "XdtjUkaMN3XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we apply SARIMA for the load time series considering its covariates. SARIMA with exogenous variables also called SARIMAX."
      ],
      "metadata": {
        "id": "EW1BM1E_Qj6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SARIMA for univariate time series\n",
        "arima = ARIMA(train_load.to_numpy(), order=(4,1,2), seasonal_order=(1, 1, 1, 24), exog=train_exog.to_numpy());\n",
        "fitted_arima = arima.fit(method_kwargs={'maxiter':10});\n",
        "\n",
        "# Perform forecasting on the training set\n",
        "forecasted = fitted_arima.predict(start=len(train_load), end=len(data)-1, exog=test_exog.to_numpy());\n",
        "plot_and_report_accuracy_metrics(test_load.to_numpy(), forecasted, 7);"
      ],
      "metadata": {
        "id": "auvN0SbCN3UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "glz5tQYX8XiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>5. XGBoost </font>** <a name=\"xgboost\"></a>\n",
        "XGBoost (eXtreme Gradient Boosting) is a powerful machine learning algorithm that can be used for forecasting applications. It combines multiple weak predictive models (e.g., decision trees) into to form a stonger model. Compared to traditional gradient boosting models, XGBoost is known for its high accuracy, computational efficiency, and enahnced regularization. Like many other forecasting methods, XGBoost assumes stationarity of the time series data. Luckily, the dataset used here is almost stationary, as out analysis showed in the previous section."
      ],
      "metadata": {
        "id": "s4II7FB11ghY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n"
      ],
      "metadata": {
        "id": "p8kZeThuMKl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% create XGBoos model\n",
        "xgr = xgb.XGBRegressor(base_score=0.5,\n",
        "                       booster='gbtree',\n",
        "                       n_estimators=1000,\n",
        "                       objective='reg:linear',# linear, squarederror\n",
        "                       max_depth=5,\n",
        "                       learning_rate=0.01);\n",
        "\n",
        "\n",
        "xgr.fit(train_exog, train_load,\n",
        "        eval_set=[(train_exog, train_load), (test_exog, test_load)],\n",
        "        verbose = False);\n",
        "\n",
        "forecasted = xgr.predict(test_exog);\n",
        "plot_and_report_accuracy_metrics(test_load.to_numpy(), forecasted, 7);"
      ],
      "metadata": {
        "id": "vnP3zf8qN3OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current XGBoost model performs poorly, perhaps not surprisingly because we know the data has seasonality which is a sign of non-stationary time series. However, there are a few ideas that can enhance the performance:\n",
        "- Hyperparameter tuning: XGBoost comes with multitude of parameters some of which, such as the ones we considered in the initial model, can have a significant impact on its performance.\n",
        "\n",
        "- Feature engineering: the current exogenous variables are basis. We can add new features such as lagged values of load and interaction terms (irradiance*wind speed). The lagged values seems perticularly useful as we already aware of the cyclic nature of the load and strong autocorrelation with closer data points. This may save us from apply de-seasonalizing the dataset."
      ],
      "metadata": {
        "id": "LUKSQYOmS9ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add lagges to the dataset\n",
        "def create_lagged_features(data, target_col, lags):\n",
        "    data_lagged = data.copy()\n",
        "    for lag in lags:\n",
        "        data_lagged[f'{target_col}_lag_{lag}'] = data_lagged[target_col].shift(lag)\n",
        "    # fill the nan values with average load\n",
        "    return data_lagged.fillna(data_lagged.mean())\n",
        "    # return data_lagged.dropna();\n",
        "# Define the lags to include\n",
        "lags = [1, 2, 24, 24*7] # Previous hour, 2 hours ago, same hour yesterday, same hour last week\n",
        "# add lagged features\n",
        "data_lagged = create_lagged_features(data.copy(), 'gross_load_MW', lags);"
      ],
      "metadata": {
        "id": "NetMEL6bVhsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update exogenous columns to include new lagged features and resplit the dataset\n",
        "exog_columns_lagged = exog_columns + [f'gross_load_MW_lag_{lag}' for lag in lags];\n",
        "\n",
        "# Split the data into train and test sets for both load and selected covariates\n",
        "train_size = 51*7*24; # medium term forecasting for the one week (last week) of the year\n",
        "train_load_lagged, test_load_lagged = data_lagged['gross_load_MW'][:train_size], data_lagged['gross_load_MW'][train_size:];\n",
        "train_exog_lagged, test_exog_lagged = data_lagged[exog_columns_lagged][:train_size], data_lagged[exog_columns_lagged][train_size:];\n"
      ],
      "metadata": {
        "id": "HQKZrNyWXI4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% create XGBoos model\n",
        "xgr = xgb.XGBRegressor(base_score=0.5,\n",
        "                       booster='gbtree',\n",
        "                       n_estimators=1000,\n",
        "                       objective='reg:linear',# linear, squarederror\n",
        "                       max_depth=5,\n",
        "                       learning_rate=0.01);\n",
        "\n",
        "xgr.fit(train_exog_lagged, train_load_lagged,\n",
        "        eval_set=[(train_exog_lagged, train_load_lagged), (test_exog_lagged, test_load_lagged)],\n",
        "        verbose = False);\n",
        "\n",
        "forecasted = xgr.predict(test_exog_lagged);\n",
        "plot_and_report_accuracy_metrics(test_load_lagged.to_numpy(), forecasted, 7);"
      ],
      "metadata": {
        "id": "x1A5eY_tiU9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "4agI_Ia7Q7zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>6. LSTM and GRU </font>** <a name=\"lstm\"></a>\n",
        "LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are both types of recurrent neural networks (RNNs) designed to handle long-term dependencies in sequential data. Here, we will apply LSTM and GRU to our example of predicting the power load for the next week, using a rather arbitrary configuration. To apply these concepts, we first prepare the data, split the dataset, normalize each set, and finally create the neural network architecture."
      ],
      "metadata": {
        "id": "2l2TBTgdnANB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "These models receive data as a series of *samples*, each consisting of past data points and corresponding output sequence. In time series where the data is given as a long sequence of data points (or data arrays), we can create multiple samples by the sliding windown trick to convert the data into a supervised learning task. For example, consider the sequence $3, 1, 4, 7, 2, 8, 5, 9, 6$. Given the window size of 4, the input samples are:\n",
        "\n",
        "sample 1: $\\quad X[0] = [3, 1, 4, 7], \\qquad y[0]= 2$<br>\n",
        "sample 2: $\\quad X[1] = [1, 4, 7, 2], \\qquad y[1]= 8$<br>\n",
        "sample 3: $\\quad X[2] = [4, 7, 2, 8], \\qquad y[2]= 5$<br>\n",
        "sample 4: $\\quad X[3] = [7, 2, 8, 5], \\qquad y[3]= 9$<br>\n",
        "sample 5: $\\quad X[3] = [2, 8, 5, 9], \\qquad y[4]= 6$<br>\n",
        "\n",
        "where $X$ is the input variables and $y$ is the label values (outputs). Once the samples are ready, the dataset is divided into training, validation, and test sets. The validation set is used for tuning the model parameters to avoid overfitting. As before, the test set is the dataset not seen by the model and is used by permormance measures.\n",
        "\n",
        "The next step is to normalize the label values. The normalization step is importance to help the model train faster, and avoid numerical instabilities.\n"
      ],
      "metadata": {
        "id": "NtfuJaLsUS3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "zYLFfNTagZRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the sequence\n",
        "X = list(); y = list();\n",
        "sliding_window_size = 4;\n",
        "yX = data[['gross_load_MW']+ exog_columns].to_numpy();\n",
        "for i in range(len(data)-sliding_window_size):\n",
        "    slide = [s for s in yX[i:i+sliding_window_size]];\n",
        "    X.append(slide);\n",
        "    y.append(data['gross_load_MW'].iloc[i+sliding_window_size]);\n",
        "X = np.array(X); y = np.array(y);\n",
        "print(X.shape, y.shape);"
      ],
      "metadata": {
        "id": "Y4I3QzV3mFu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train/validation/test split\n",
        "train_size = 50*7*24;\n",
        "val_size = 7*24;\n",
        "y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:];\n",
        "X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:];\n",
        "y_train.shape, y_test.shape, X_train.shape, X_test.shape, y_val.shape, X_val.shape"
      ],
      "metadata": {
        "id": "b9MSvTmumJiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the label values\n",
        "# X_train_mean = np.mean(X_train[:,:,0]);X_test_mean = np.mean(X_test[:,:,0]);\n",
        "# X_train_std = np.std(X_train[:,:,0]);X_test_std = np.std(X_test[:,:,0]);\n",
        "\n",
        "# y_train_mean = np.mean(y_train);y_test_mean = np.mean(y_test);\n",
        "# y_train_std = np.std(y_train);y_test_std = np.std(y_test);\n",
        "\n",
        "# X_train[:,:,0] = (X_train[:,:,0]-X_train_mean)/X_train_std;\n",
        "# X_test[:,:,0] = (X_test[:,:,0]-X_test_mean)/X_test_std;\n",
        "# y_train_normalized = (y_train - y_train_mean) / y_train_std;\n",
        "# y_test_normalized = (y_test - y_test_mean) / y_test_std;\n",
        "\n",
        "# normalize the label values\n",
        "y_train_mean = np.mean(y_train); y_val_mean = np.mean(y_val); y_test_mean = np.mean(y_test);\n",
        "y_train_std = np.std(y_train); y_val_std = np.std(y_val); y_test_std = np.std(y_test);\n",
        "\n",
        "X_train_mean = np.mean(X_train, axis=(0, 1)); X_val_mean = np.mean(X_val, axis=(0, 1)); X_test_mean = np.mean(X_test, axis=(0, 1));\n",
        "X_train_std = np.std(X_train, axis=(0, 1)); X_val_std = np.std(X_val, axis=(0, 1)); X_test_std = np.std(X_test, axis=(0, 1));\n",
        "\n",
        "X_train = (X_train - X_train_mean) / X_train_std; X_val = (X_val - X_val_mean) / X_val_std; X_test = (X_test - X_test_mean) / X_test_std;\n",
        "y_train = (y_train - y_train_mean) / y_train_std; y_val = (y_val - y_val_mean) / y_val_std;"
      ],
      "metadata": {
        "id": "YsWa85XJyqgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start with a simple model\n",
        "\n",
        "Model = Sequential();\n",
        "Model.add(InputLayer((sliding_window_size, len(exog_columns)+1)));\n",
        "Model.add(LSTM(30));\n",
        "Model.add(Dense(1, 'linear'));\n",
        "Model.summary();\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "cp_lstm = ModelCheckpoint(\n",
        "    filepath='/tmp/checkpoint/LSTM.keras',\n",
        "    save_weights_only=False,  # Save the entire model\n",
        "    # monitor='val_accuracy',   # Monitor validation accuracy\n",
        "    # mode='max',               # Look for maximum validation accuracy\n",
        "    save_best_only=True,      # Only save the best model\n",
        "    # verbose=1                 # Display messages when saving\n",
        "    );\n",
        "\n",
        "Model.compile(optimizer=Adam(learning_rate=0.005), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()]);\n",
        "Model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks=[cp_lstm]);\n",
        "# Load the best model and forecast\n",
        "best_model = tf.keras.models.load_model('/tmp/checkpoint/LSTM.keras');\n",
        "# Make forecast and inverse transform\n",
        "forecasted_normalized = best_model.predict(X_test);\n",
        "forecasted = np.array((forecasted_normalized * y_test_std) + y_test_mean);\n",
        "\n",
        "plot_and_report_accuracy_metrics(y_test, forecasted, 7);\n",
        "y_test.shape, forecasted.shape"
      ],
      "metadata": {
        "id": "bQJN5E61yZ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model = Sequential();\n",
        "Model.add(InputLayer((sliding_window_size, len(exog_columns)+1)));\n",
        "Model.add(LSTM(32, return_sequences=True));\n",
        "Model.add(Dropout(0.2));\n",
        "Model.add(LSTM(64));\n",
        "Model.add(Dropout(0.2));\n",
        "Model.add(Dense(8, 'relu'));\n",
        "Model.add(Dense(1, 'linear'));\n",
        "Model.summary();\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "cp_lstm = ModelCheckpoint(\n",
        "    filepath='/tmp/checkpoint/LSTM.keras',\n",
        "    save_weights_only=False,  # Save the entire model\n",
        "    # monitor='val_accuracy',   # Monitor validation accuracy\n",
        "    # mode='max',               # Look for maximum validation accuracy\n",
        "    save_best_only=True,      # Only save the best model\n",
        "    # verbose=1                 # Display messages when saving\n",
        "    );\n",
        "\n",
        "Model.compile(optimizer=Adam(learning_rate=0.005), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()]);\n",
        "Model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks=[cp_lstm]);\n",
        "# Load the best model and forecast\n",
        "best_model = tf.keras.models.load_model('/tmp/checkpoint/LSTM.keras');\n",
        "# Make forecast and inverse transform\n",
        "forecasted_normalized = best_model.predict(X_test);\n",
        "forecasted = np.array((forecasted_normalized * y_test_std) + y_test_mean);\n",
        "plot_and_report_accuracy_metrics(y_test, forecasted, 7);"
      ],
      "metadata": {
        "id": "V3uEBJzmxX_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU\n",
        "Model = Sequential();\n",
        "Model.add(InputLayer((sliding_window_size, len(exog_columns)+1)));\n",
        "Model.add(GRU(32));\n",
        "Model.add(Dense(1, 'linear'));\n",
        "Model.summary();\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "cp_lstm = ModelCheckpoint(\n",
        "    filepath='/tmp/checkpoint/LSTM.keras',\n",
        "    save_weights_only=False,  # Save the entire model\n",
        "    # monitor='val_accuracy',   # Monitor validation accuracy\n",
        "    # mode='max',               # Look for maximum validation accuracy\n",
        "    save_best_only=True,      # Only save the best model\n",
        "    # verbose=1                 # Display messages when saving\n",
        "    );\n",
        "\n",
        "Model.compile(optimizer=Adam(learning_rate=0.005), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()]);\n",
        "Model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks=[cp_lstm]);\n",
        "# Load the best model and forecast\n",
        "best_model = tf.keras.models.load_model('/tmp/checkpoint/LSTM.keras');\n",
        "# Make forecast and inverse transform\n",
        "forecasted_normalized = best_model.predict(X_test);\n",
        "forecasted = np.array((forecasted_normalized * y_test_std) + y_test_mean);\n",
        "plot_and_report_accuracy_metrics(y_test, forecasted, 7);"
      ],
      "metadata": {
        "id": "6xH5D1lwi6EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU + LSTM\n",
        "Model = Sequential();\n",
        "Model.add(InputLayer((sliding_window_size, len(exog_columns)+1)));\n",
        "Model.add(LSTM(60, return_sequences=True));\n",
        "Model.add(Dropout(0.2));\n",
        "Model.add(Conv1D(20, kernel_size=3));\n",
        "Model.add(GRU(50));\n",
        "Model.add(Dropout(0.2));\n",
        "Model.add(Dense(8, 'relu'));\n",
        "Model.add(Dense(1, 'linear'));\n",
        "Model.summary();\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "cp_lstm = ModelCheckpoint(\n",
        "    filepath='/tmp/checkpoint/LSTM.keras',\n",
        "    save_weights_only=False,  # Save the entire model\n",
        "    # monitor='val_accuracy',   # Monitor validation accuracy\n",
        "    # mode='max',               # Look for maximum validation accuracy\n",
        "    save_best_only=True,      # Only save the best model\n",
        "    # verbose=1                 # Display messages when saving\n",
        "    );\n",
        "\n",
        "Model.compile(optimizer=Adam(learning_rate=0.005), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()]);\n",
        "Model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks=[cp_lstm]);\n",
        "# Load the best model and forecast\n",
        "best_model = tf.keras.models.load_model('/tmp/checkpoint/LSTM.keras');\n",
        "# Make forecast and inverse transform\n",
        "forecasted_normalized = best_model.predict(X_test);\n",
        "forecasted = np.array((forecasted_normalized * y_test_std) + y_test_mean);\n",
        "plot_and_report_accuracy_metrics(y_test, forecasted, 7);"
      ],
      "metadata": {
        "id": "gTPb0lWq2Jv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "06pvu6d80cVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>7. Case Study Questions </font>** <a name=\"questions\"></a>\n",
        "\n",
        "There are two main activities for you to cover the medium and long-term forecasting of power demand. The first question wants you to determine the performance of the aforementioned techniques on a unit commitment (UC) problem.\n",
        "\n",
        "## Medium-Term Forecasting\n",
        "\n",
        "So far, we have examined the performance of the forecasting methods based on traditional RSME, MAE, and MAPE.\n",
        "As mentioned in the lecture, the projected values are usually used in some downstream planning tasks, especially in the context of energy and sustainability. Therefore, it is better to use these metrics in conjuctino with a measure that indicates their performance on the actual task for which they are intended. For example, overestimation of load is less detrimental than underestimation in day-ahead planning as the latter may result in costly blackouts whereas the former may only require extra less costly reserve generators.\n",
        "Here, the planning problem of interest is UC problem implemented in the following cell. UC common optimization model that determines the cost-effective hourly schedule for generators over the planning horizon (more on UC in later case studies).\n",
        "\n",
        "**<font color='red'> Questions </font>**\n",
        "1. Assume that the forecast period is 2 days (previously a week). Apply SARIMA, XGBoost, and LSTM with your own choice of hyperparameters to project the demand for the last 48 hours of the year. Rank the techniques by their performance.\n",
        " Evaluate the forecasted as well as values in the test set on the UC problem. Let $\\hat{z}$ and $z$ be the resuling objective function values for the forecasted and actual demand. Compute the gap as $\\frac{|\\hat{z}-z|}{z}$. Instead of point accuracy measures, how would your ranking change if you compare the techniques based on the resulting gap?\n",
        "\n",
        "2. The forecasting techniques are trained based on their loss function which is usually a function of the forecasted values and actual values in the training set. In light of previous question, do you think training these algorithms with two loss functions, one internal and the other from the downstream task, is a good idea? How do you think this is possible? Explain your answer without being too rigorous. The goal is to encourage brainstorming, exploring different options, and eliciting a cohesive answers.\n",
        "\n",
        "<br>\n",
        "<br>values\n",
        "<br>\n",
        "\n",
        " ## Long-term Forecasting\n",
        "Long-term forecasting in energy application is essential to prepare the infrastructure, evaluate the various policies, and assess the impact of different technologies.\n",
        "\n",
        "**<font color='red'> Questions </font>**\n",
        "\n",
        "1. The dataset folder 'load_time_series_data' contains the historical load for ISONE. Consider the data from 2000 to 2023 (24 years). Your task is to project the load for the year 2050 assuming 1\\% annual load growth. To carry out this task:\n",
        "\n",
        " -  Read each file from 2000 to 2021. Consider the data for the years 2022 and 2023 as validation and test sets.\n",
        " - Add 'year' column to each file and concatenate them all to form the training dataset with about 23*8760 rows (some years are leap, so the number of rows will be slightly more).\n",
        " - Similar to the data preparation in the LSTM section, prepara the data with different window size (6, 12, 24, 168).\n",
        " - Train LSTM with the configuration of your choice.\n",
        " - Predict for the year 2023 and assess its accuracy.\n",
        " - Inflate the forecasted values considering the annual growth rate to obtain the projection for the target future year.\n",
        "\n",
        "2. The projection carried out in the previous question ignores many factors that can influence the distribution of the demand by 2050. Some of these factors inclue climate change, electrification of end-use (more electric vehicles, heat pumps instead of gas furnace, etc.), population growth, electric-intensive industry (computing and datacenters), among others. How do you think some of these factors can be incorporated in the forecasting? Again, explain your answer without being too rigorous.\n"
      ],
      "metadata": {
        "id": "7b3B8DFm7aiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Commitment Problem\n",
        "Make sure to install the required package as instructured in the following."
      ],
      "metadata": {
        "id": "mStApOx9z0CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment and install as follow\n",
        "!pip install pyoptinterface[highs]"
      ],
      "metadata": {
        "id": "HCg_7yXVzVgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyoptinterface as poi;\n",
        "from pyoptinterface import highs;\n",
        "\n",
        "def UC(demand):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    demand : 1-dimensional numpy array\n",
        "\n",
        "    this function solves a simple unit commitment problem for a given demand parameter.\n",
        "\n",
        "    OUTPUT: objective function value\n",
        "    '''\n",
        "    demand = demand.reshape(-1);\n",
        "    nGen = 15;\n",
        "    nT = len(demand); # one week, hourly resolution\n",
        "\n",
        "    # defien generator parameters\n",
        "    Pmax = np.random.randint(int(0.6*demand.max()/nGen), int(1.6*demand.max()/nGen), size=nGen);\n",
        "    Pmin = np.zeros(nGen);\n",
        "    RampU = np.random.uniform(low=0.2, high=0.7, size=nGen);\n",
        "    RampD = np.random.uniform(low=0.1, high=0.5, size=nGen);\n",
        "    startup_cost = np.random.randint(10, 30, size=nGen);\n",
        "    vom = np.random.randint(1, 5, size=nGen);\n",
        "    initial_status = np.random.randint(0, 2, size=nGen);\n",
        "    load_shedding_cost = 1e4; #typically between 10k and 30k per MWh\n",
        "\n",
        "    # initialize the model\n",
        "    Model = highs.Model();\n",
        "\n",
        "    # define decison variables\n",
        "    class DV():u=[]; p=[]; us=[]; shed=[];\n",
        "\n",
        "    DV.u = Model.add_variables(range(nGen), range(nT), lb = 0,domain=poi.VariableDomain.Binary); #unit commitment\n",
        "    DV.p = Model.add_variables(range(nGen), range(nT), lb = 0,domain=poi.VariableDomain.Continuous); # generation/production\n",
        "    DV.su = Model.add_variables(range(nGen), range(nT), lb = 0,domain=poi.VariableDomain.Binary); # startup\n",
        "    DV.shed = Model.add_variables(range(nT), lb = 0, domain=poi.VariableDomain.Continuous); # load shedding\n",
        "\n",
        "    # define and add the objective function\n",
        "    obj = poi.ExprBuilder();\n",
        "    for t in range(nT):\n",
        "        for g in range(nGen):obj += vom[g]*DV.p[g,t]+startup_cost[g]*DV.su[g,t];\n",
        "        obj += load_shedding_cost*DV.shed[t];\n",
        "    Model.set_objective(obj, poi.ObjectiveSense.Minimize);\n",
        "\n",
        "    # constraints\n",
        "    # balance equation\n",
        "    for t in range(nT):Model.add_linear_constraint(poi.quicksum(DV.p[g,t] for g in range(nGen))+DV.shed[t], poi.Eq, demand[t]);\n",
        "\n",
        "    # generation limit,  startup and ramping\n",
        "    for g in range(nGen):\n",
        "        for t in range(nT):\n",
        "            Model.add_linear_constraint(DV.p[g,t], poi.Leq, Pmax[g]); # gen upper bound\n",
        "            Model.add_linear_constraint(DV.p[g,t], poi.Geq, Pmin[g]); # gen lower bound\n",
        "            if t>0:\n",
        "                Model.add_linear_constraint(DV.su[g,t]-DV.u[g,t]+DV.u[g,t-1], poi.Geq, 0); # startup definition\n",
        "                Model.add_linear_constraint(DV.p[g,t]-DV.p[g,t-1], poi.Leq, RampU[g]*Pmax[g]); # ramp up\n",
        "                Model.add_linear_constraint(DV.p[g,t-1]-DV.p[g,t], poi.Leq, RampD[g]*Pmax[g]);# ramp down\n",
        "\n",
        "    # solve and post-process\n",
        "    Model.optimize();\n",
        "\n",
        "    # calculate total generation and load shedding\n",
        "    total_gen, total_unserved_power = 0, 0;\n",
        "    for t in range(nT):\n",
        "        for g in range(nGen): total_gen += Model.get_value(DV.p[g,t]);\n",
        "        total_unserved_power += Model.get_value(DV.shed[t]);\n",
        "\n",
        "    print(f'total generation: {round(total_gen)} MWh, \\t total unserved power: {round(total_unserved_power)} MWh');\n",
        "    # print(Model.get_model_attribute(poi.ModelAttribute.TerminationStatus));\n",
        "    return np.round(Model.get_value(obj),2);"
      ],
      "metadata": {
        "id": "6iPJbowWz9e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE\n",
        "demand = data['gross_load_MW'].iloc[51*24:53*24].to_numpy();\n",
        "obj = UC(demand)\n",
        "\n"
      ],
      "metadata": {
        "id": "pUZBb3pSyxgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "_W77HimmujqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='blue'>8. References and Further Reading </font>** <a name=\"references\"></a>\n",
        "\n",
        "**Main references used in the tutorial portion of this case study:**\n",
        "\n",
        "- Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: principles and practice. OTexts. ([link to the book](https://www.amazon.com/dp/0987507133?tag=otexts20), [website](https://otexts.com/fpp3/))\n",
        "- Haben, S., Voss, M., & Holderbaum, W. (2023). Core concepts and methods in load forecasting: With applications in distribution networks (p. 331). Springer Nature. ([link to the book](https://link.springer.com/book/10.1007/978-3-031-27852-5))\n",
        "- XGBoost Documentation ([website](https://xgboost.readthedocs.io/en/stable/index.html))\n",
        "- Brownlee, J. (2018). Deep learning for time series forecasting: predict the future with MLPs, CNNs and LSTMs in Python. Machine Learning Mastery ([link to the book](https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/))\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "From the vast topic of forecasting with its extensive literature, this case study only concers with a few techniques in point forecasting of power demand.  Apart from the main references mention above, here are a few resources for **further reading** .\n",
        "\n",
        "- Petropoulos, F., Apiletti, D., Assimakopoulos, V., Babai, M. Z., Barrow, D. K., Taieb, S. B., ... & Ziel, F. (2022). Forecasting: theory and practice. International Journal of forecasting, 38(3), 705-871. [link](https://www.sciencedirect.com/science/article/pii/S0169207021001758)\n",
        "- Alkhayat, G., & Mehmood, R. (2021). A review and taxonomy of wind and solar energy forecasting methods based on deep learning. Energy and AI, 4, 100060. [link](https://www.sciencedirect.com/science/article/pii/S2666546821000148)\n",
        "\n",
        "- Hewamalage, H., Ackermann, K., & Bergmeir, C. (2023). Forecast evaluation for data scientists: common pitfalls and best practices. Data Mining and Knowledge Discovery, 37(2), 788-832. [link](https://arxiv.org/pdf/2203.10716)\n",
        "- Elmachtoub, A. N., & Grigas, P. (2022). Smart “predict, then optimize”. Management Science, 68(1), 9-26. [link](https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.2020.3922)\n"
      ],
      "metadata": {
        "id": "98U8UxjZ7afs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "Co6vbP8j6D0f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QUXw9yO6E0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q6S3oUwj6OVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HOIQGMy6OSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQMJb8n56OPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}